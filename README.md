# CPU-Only RAG Pipeline

A fast, production-ready Retrieval-Augmented Generation (RAG) system running fully on CPU.  
Supports dense retrieval with FAISS and quantized TinyLlama GGUF LLM server (llama.cpp).

---

## Features

- 100% CPU-only (no GPU needed)  
- FAISS for dense semantic retrieval  
- sentence-transformers/all-MiniLM-L6-v2 for embeddings  
- TinyLlama-1.1B (Q4_K_M GGUF) for local LLM  
- FastAPI backend with `/query` REST endpoint  
- Live benchmarking & source attribution  

---

## How To Run This Project (Step-by-Step)

### 1. Clone and install requirements

git clone https://github.com/mandar-meshram/CPU_RAG_pipeline.git
cd CPU_RAG_pipeline
python3 -m venv env
source env/bin/activate
pip install -r requirements.txt


### 2. Download the quantized model (1st time only)
mkdir -p models
curl -L -o models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf



### 3. Start the LLM server (terminal 1)
python3 -m llama_cpp.server
--model models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
--host 127.0.0.1 --port 8080 --n_gpu_layers 0


You should see a message like:  
`ðŸš€ LLM server ON 8080`

### 4. Start the FastAPI server (terminal 2)
uvicorn app.main:app --reload


### 5. Run queries (terminal 3) Example using curl:
curl -X POST "http://127.0.0.1:8000/query"
-H "Content-Type: application/json"
-d '{"query": "What is FAISS?"}'


---

## Benchmarking & Performance

- Typical end-to-end latency: **6-10ms** on MacBook M4 (CPU-Only).  
- Retrieval accuracy: Relevant document chunks appear reliably in top-K results.  
- Latency automatically measured and returned with each query.

---

## Deployment Guide

- All processes (retrieval, embedding, LLM) run on CPU.  
- Can be containerized with Docker or managed via systemd; sample Dockerfile available.  
- Efficient resource use: ~700MB RAM, <10% CPU load on modern CPUs.

---





